%%%%%% CMB-S4 Simulations and Data Analysis Chapter, Time-Ordered Data Processing Section  %%%%%%%%%%%%%%%%
 
\section{Time-Ordered Data Processing}

This section discusses the first stage of the CMB data analysis pipeline, in which the raw time-ordered data from every detector are first pre-processed and then combined into an estimate of the temperature and polarization on the sky. We discuss those steps---and some of the challenges we expect to face in implementing those steps in the \cmbexp\ era---below.

\subsection{Pre-Processing and Mission Characterization}

The first stage of analysis in typical CMB experiments involves pre-processing the raw 
time-ordered data in an attempt to clean the data of time-domain systematics and 
make the real data match a model that will underpin all subsequent analyses. Typical 
steps in this pre-processing are finding and removing cosmic-ray hits (``glitches'') on individual detectors
and narrow-band filtering of spectral-line-like contamination to the time-ordered data (often 
from detector sensitivity to a mechanical apparatus such as the cryocooler). 
A challenge in the \cmbexp\ era will be to properly account for these steps---which can involve
the data from the entire set of detectors over a long observing period---in the time-ordered data
simulation pipeline, in order to characterize their effects on the final science results. The data 
volume is sufficiently large at this step that multiple full simulations may be unfeasible.

The pre-processing phase is also where many of the inputs to the mission model (a key piece of the data simulation pipeline) are measured. These inputs include the overall observation pointing reconstruction, and the individual detector beam profiles, bandpasses, and noise charateristics, complementing dedicated instrument-characterizing observations and other laboratory or field measurements.


\subsection{Map-Making}
\label{sec_mapmaking}

Map-making is the stage of the analysis when the major compression of the time-ordered data happens and some estimate of the sky signal is produced at each observing frequency. It is usually a linear operation, characterized by some operator, $\mathbf{L}$, which transforms the input time-ordered data, $\mathbf{d}$, into a pixel domain map, $\mathbf{m}$, 
e.g., \cite{Tegmark:1996qs},
\begin{eqnarray}
\mathbf{m} = \mathbf{L}\mathbf{d},
\end{eqnarray}
typically under the condition that the estimator is unbiased over the statistical ensemble of instrumental noise realizations, i.e.,
\begin{eqnarray}
\langle \mathbf{m} - \mathbf{s}\rangle = 0,
\label{eq:condMaps}
\end{eqnarray}
where $\mathbf{s}$ is the underlying pixelized sky signal. Given the usual model for the time-ordered data as the sum of sky-synchronous signal and time-varying noise, 
\begin{eqnarray}
\mathbf{d} = \mathbf{A}\mathbf{s} + \mathbf{n},
\end{eqnarray}
for a pointing matrix $\mathbf{A}$, this condition leads to,
\begin{eqnarray}
\langle \mathbf{m} - \mathbf{s}\rangle =  (\mathbf{L}\mathbf{A}-\mathbf{1})\mathbf{s} 
+ \langle \mathbf{n} \rangle = (\mathbf{L}\mathbf{A}-\mathbf{1})\mathbf{s},
\end{eqnarray}
as the average noise is assumed to vanish. Hence,
\begin{eqnarray}
\mathbf{L}\mathbf{A} = \mathbf{1},
\end{eqnarray}
which is solved by,
\begin{eqnarray}
\mathbf{L} = (\mathbf{A}^{\rm T} \mathbf{W} \mathbf{A})^{-1} \mathbf{A}^{\rm T} \mathbf{W}.
\end{eqnarray}
Here the matrix $\mathbf{W}$ is an arbitrary positive definite weight matrix, and different choices of $\mathbf{W}$ lead to different estimates of the sky signal.
\begin{itemize}
\item If $\mathbf{W}$ is taken to be the inverse of the time-domain noise covariance, i.e., $\mathbf{W} = \mathbf{N}^{-1}$, then the sky signal estimate, $\mathbf{m}$, will correspond to the {\bf maximum likelihood} and {\bf minimum variance} solution. 
\item If $\mathbf{W}$ is taken to be proportional to some diagonal matrix minus some low-rank correction, i.e. $\mathbf{W} \propto \mathbf{1} - \mathbf{T}\mathbf{T}^{\rm T}$,  with $\mathbf{T}$ assumed to be column-orthogonal, then the modes defined by its columns are marginalized over, effectively removing them from the solution. This approach includes as a special case so-called {\bf destriping} map-making, e.g.,~\cite{Poutanen:2004hy, Keihanen:2003pu}, which has gained recognition thanks to its successful applications to the \planck\ data, e.g.,~\cite{Keihanen:2009tj, Tristram:2011gq, Ade:2015uua, Adam:2015vua}, and is therefore of potential interest to any experiments aiming to cover a large fraction of the sky. More generally, however $\mathbf{T}$ can be constructed to remove any unwanted modes present in the time domain data, e.g.,~\cite{Stompor:2002jy, Cantalupo:2009if, Dunner:2012vp}.
\item If $\mathbf{W}$ is taken to be diagonal, then the map-making solution corresponds to {\bf binning}, i.e. the weighted co-addition of the samples falling within each pixel.
\end{itemize}
If the instrument beams display complex, non-axially symmetric structure, the proper estimation of the sky signal may require correcting for their effects at the map level, leading to the so-called {\bf deconvolution} map-making ~\cite{Armitage:2004pk, Harrison:2011xt, Keihanen:2012rm}.  However, further work is needed to demonstrate the effectiveness of such an approach in general.

If map-making is used primarily as a data compression operation on the way to deriving constraints on the statistical properties of the sky signal (such as its power spectra), one may choose to relax the condition in Eq.~(\ref{eq:condMaps}) in favor of the more computationally tractable, albeit potentially biased, sky estimate,
\begin{eqnarray}
\mathbf{m} = (\mathbf{A}^{\rm T} {\rm diag}( \mathbf{W}) \mathbf{A})^{-1} \mathbf{A}^{\rm T}\mathbf{W} \mathbf{d},
\end{eqnarray}
where ${\rm diag}( \mathbf{W})$ denotes the diagonal part of $\mathbf{W}$. In this approach any bias is then corrected at the next level of the data processing, e.g.,~\cite{Hivon:2001jp}. This approach has been proven to be very effective, at least in the context of experiments with small sky coverage, e.g., \cite{Culverhouse:2010ya, Schaffer:2011mz, Ade:2014afa, Ade:2014xna}.

Formally the linearity of the mapmaking operation permits the propagation of the uncertainty due to the instrumental noise from time- to pixel-domain as
\begin{eqnarray}
{\hat{\mathbf{N}}} = \mathbf{L} \mathbf{N} \mathbf{L}^{\rm T},
\end{eqnarray}
which leads to a particularly simple expression for maximum likelihood estimators 
\begin{eqnarray}
{\hat{\mathbf{N}}} = (\mathbf{A}^{\rm T} \mathbf{N}^{-1} \mathbf{A})^{-1}.
\end{eqnarray}
However, as noted above, the computational cost of computating such pixel-domain noise correlation matrices make it impractical for all but special cases today, and the uncertainty is carried over to the next stages of the data processing in implicit form and ultimately estimated using Monte Carlo simulations.
